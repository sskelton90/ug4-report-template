\chapter{Introduction}
With the popularity and usage of social networking rapidly increasing every day there are exabytes upon exabytes of data available for analysis and research. The data available to anyone with an Internet connection includes text, pictures, and videos; virtually anything that can be uploaded onto the Internet.

Twitter is one particular social network where users post 140 character messages called Tweets. Users can share messages, links, images, and videos with anyone in the world who wishes to subscribe to their updates. Twitter users alone posted over 95 million Tweets per day in 2011[CITE] so there is a huge amount of text data that can be used for research purposes.

One area where examples of language data are incredibly useful is in the area of machine-based language research. Possible uses for social networking data in this area include increasing accuracy of machine translation systems, predicting stock prices based on the language people are using at a particular time or even predicting a user's personality based on the language they use. The problem with the text data used in these systems, gained from social networking, is that a lot of it has many spelling errors. These spelling errors, if not corrected, will reduce the overall accuracy of the systems based on the social networking data.

This project aims to create a spelling error correction system tailored to maximise accuracy in correcting the spelling of Tweets collected from the social network Twitter.

\section{Prior Work}
Modern spelling error correction systems can be categorised into solving three sub-problems \cite{Kukich1992}:

\begin{itemize}
\item
Non-word error detection
\item
Isolated-word error correction
\item
Context-dependent error detection and correction
\end{itemize}

In this section, each of the sub-problems are discussed in order to summarise how spelling error detection and correction is done today in general and the problems caused by features specific to Tweets.

\subsection{Non-word Error Detection}

\subsubsection{Detection}

Non-word spelling errors occur when an error causes the word to become a word that doesn't exist in the given language. For example, the non-word \emph{dymanite} might result from a user mistyping the word \emph{dynamite}.

To detect non-word spelling errors a simple technique of dictionary lookup is used. The naive idea behind using dictionary lookup is: if a given word cannot be found in the dictionary then the word must be spelled incorrectly.

Using only dictionary lookup for spelling error detection causes a number of problems: 

\begin{itemize}
\item
A correctly spelled word may be flagged as incorrectly spelled if the word is not present in the particular dictionary.
\item
An incorrectly spelled word may not be flagged as incorrect if the misspelling is also present in the dictionary.
\item
To cover the many words in a language, large dictionaries are necessary which require a great amount of storage space and slow down lookup time.
\end{itemize} 

To solve these problems dictionary choice and their content is key. In 1990, Damerau proposed a technique for automatically generating a domain specific dictionary by using large quantities of text from a particular subject domain \cite{Damerau1990}. Frequency analysis is then performed on the text and a subset with the highest number of occurrences is selected as the dictionary for that subject. Damerau concluded that this was an effective way to produce a domain specific and size-constrained dictionary to ensure high accuracy and low response times on lookup. Other methods to ensure fast lookup times are the use of hash-tables and tries. To solve the problem of misspellings also being present as words in the dictionary context-sensitive spelling error detection can be used and this is discussed later.

\subsection{Isolated-word Error Correction}

Isolated-word error correction is the correction of words without taking the word's context into consideration. For example, it is possible to correct the word \emph{dymanite} to \emph{dynamite} without looking at the position of the word in the sentence as there are no context-sensitive uses of the word \emph{dynamite}. Isolated-word spelling error correction is usually performed using two complementary techniques: calculating the minimum edit distance of a misspelled word to a correctly spelled word and the Bayesian-based noisy channel model.

\subsubsection{Minimum Edit Distance Method}

To generate a set of candidate spelling error corrections for the isolated word, a common algorithm to use is the minimum edit distance algorithm. To calculate the minimum edit distance between two strings scores are assigned to the three possible actions that can transform one string into another; these actions are insertion, deletion, and substitution. The minimum edit distance between two words is, therefore, the fewest number of actions to transform one string into another. It is possible to use this algorithm to find possible corrections to spelling errors as it is likely that the user has only made a small error in their input.

The disadvantages of using minimum edit distance are the speed of the calculation itself and the need to check every word in the dictionary against the misspelled word. The speed of calculation can be made more efficient by implementing a dynamic programming version of the algorithm. Dynamic programming splits the large problem into smaller sub-problems and stores the results of the sub-problems in a table so that they do not need to be calculated again. By keeping the dictionary sizes small, it is possible to overcome the problem of inefficiency in requiring to check the misspelling against each word in the dictionary.

\subsubsection{Noisy Channel Model Method}

Once the set of candidate spelling corrections have been found, the noisy channel model can be used to select between the candidates, using Bayesian inference, to find the word that is most likely the word the user was intending to input. The noisy channel model for spelling error correction was first proposed by Kernighan in 1990 \cite{Kernighan1990} and is still in wide use today. Finding the most probable correction is calculated using the equation:
\[
\hat{c} = \arg\max_{\forall c \in C} P(t|c) P(c)
\]
where C is the set of possible corrections and t is the incorrectly spelled word.

To estimate the term P(c), named the prior probability, one can simply count the occurrences of the word in a large domain specific text. Estimating the prior probability this way means that the most common words in the text are more probable and the least common words have probability close to zero. Obviously, if the word the user meant to input is uncommon then this can cause the spelling error correction system to suggest the wrong word. For this reason, spelling correction systems today offer choice between corrections or use corpora of N-grams to offer more accurate results.

Estimating the likelihood parameter, P(t\textbar c), for each possible correction is done by creating a confusion matrix that stores the number of times one incorrect letter was input instead of the correct letter. It is possible to create this confusion matrix using machine learning techniques e.g. iteration as proposed by Kernighan.\cite{Kernighan1990} Learning the confusion matrix by iteration involves setting all initial values in the matrix to be the same. From there the algorithm is run iteratively on a data set of spelling mistakes, with the mistakes compared to the corrections returned by the system.

\subsubsection{Character-level N-gram Method}

The use of character-level n-grams for spelling error correction was first proposed in 1983 by E. J. Yannakoudakis and D. Fawthrop. Character-level n-grams split a word into all possible slices of size n. For example, the word "Twitter" can be represented as the set of size 2 n-grams: \{"Tw", "wi", "it", "tt", "te", "er"\}. The main idea behind this algorithm was to use the character-level n-grams as a similarity index between a spelling error and a correctly spelled word in an analogous way to the minimum edit distance method. To decide whether two words are similar two n-gram sets are calculated. The similarity index is then calculated using the equation (given x and y are the two n-gram sets of the two words to be compared:

\[
similarity(x,y) = \frac{\|x \cap y \|}{\|x \cup y \|}
\]

The problem with this method is that it does not take into account the ordering of the n-grams, only the presence of them. This fault means that words made up of the same n-grams will have the same similarity index when in fact one word may be more similar than the other. More recently, in 2009 Ahmed et. al. described an extension to the original method described by Yannakoudakis and Fawthrop. This method does take into account the order and placement of the individual n-grams. By comparing each n-gram in the misspelled word to n-grams in the correctly spelled word within a small window it allows for a more accurate similarity metric to be used. Ahmed et. al. found that accuracy increased dramatically when using this new metric when compared to the original as described by Yannakoudakis and Fawthrop.

\subsection{Context-dependent Error Detection and Correction}

A more accurate method of spelling error detection and correction is to use the context of each word to determine, first, whether it is spelled correctly, and secondly, which word to replace the incorrect word with. To take context into account one must look at the preceding words to decide if the word is spelled correctly and whether a correction is the right choice. For this task there are many n-gram corpora, most notably Google's 24 gigabyte 5-gram collection, which hold millions of examples of phrases with up to five constituent words. With the n-grams one can more accurately model a noisy channel, as in the preceding section, by calculating the probabilities of n-grams being present in a text instead of calculating the probabilities for individual words. Using this method gives more accurate corrections as it can decide whether a particular word is spelled correctly given the context of the word. For example, using this method allows for a spelling correction system to notice that the user has input the wrong version of \emph{there}, \emph{their}, or \emph{they're} and also correct the user's input with the correct version of \emph{there}, \emph{their}, and \emph{they're}.

\section{Twitter-specific Spelling Error Detection and Correction}

The problem of spelling error detection and correction, while not completely solved, is well researched and there exist multiple systems that can fix errors with a high degree of accuracy. The problem with using these existing systems in correcting Tweets is that users of Twitter have a specific way of using language. The biggest problem is that of contractions.

Contractions are used by members of Twitter because of the restrictions placed on the length of one Tweet; Tweets are limited at 140 characters at maximum. In order to maximise the amount of content within one Tweet users tend to shorten words by, for example, removing vowels or replacing words with their symbol counterparts. A typical example of this can be found in the Tweet:
\begin{quote}
@user2127 SO true! \& There can't be peace when ppl are playing both sides. I'm 2 convinced there's sum dbl biz going on.
\end{quote}
As is obvious from the above example of a typical Tweet; the word "and" has been replaced by the commonly-used ampersand (\&), the word "people" has been shortened to "ppl" by removing all of the vowels, "too" has been replaced by the number two, "some" has been replaced by the shorter-length homophone "sum", "double" has been shortened similarly to "people", and the word "business" has been shortened to "biz". When running this example through Aspell, a Unix spelling error corrector, the only error the software finds is "ppl" and the suggestions for corrections do not include the word "people".

Another problem with spelling error corrections on Tweets specifically is the presence of Twitter-specific features. In order to provide an easy to use and flexible system Twitter includes a few special keywords that activate functions on Tweets. These features are as follows:
\begin{itemize}
\item
@username - Typing the "@" symbol followed by the username of a Twitter user sends a public message to the user specified.
\item
#topic - Typing the "#" symbol followed by a topic (also known as a hash tag) allows users to easily find Tweets talking about a particular topic.
\item
RT - By inserting the letters "RT" allows a user to repost a Tweet that they found particularly interesting to bring it to the attention of other users.
\item
URLs - Users can link to other websites to share content such as pictures, videos, and other websites.
\end{itemize}
A solution to spelling error correction for Tweets must deal with all of these features in some way. An important question to ask is whether these are errors at all and whether they need to be corrected or are they part of the Twitter lexicon and acceptable as examples of language for use in a machine language system.

\section{Project Goals}