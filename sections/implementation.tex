\chapter{System Implementation}

To build a system capable of meeting the goals of the project it was important to design the system thoroughly from the start to ensure it would be easy to extend. To guarantee extendability allowed for many different techniques to be quickly and effectively experimented on making the final system as accuarate and as efficient as possible. This section discusses the various design decisions that were made, the reasoning behind the choices, and outlines the design of the final system that was used to meet the project objectives.

\section{High-Level System Outline}
To build a system that meets all of the objectives set out at the beginning of the project a high-level system outline was created. The system required two main steps common to all spelling error detection and correction methods. These steps were:
\begin{itemize}
	\item
	loading and processing of the training texts;
	\item
	running the spelling error detection and correction algorithms.
\end{itemize}

\subsection{Loading and Processing the Training Texts}
The first step in the Tweet spelling error corrector system is to load the training text and split them into their constituent words. To simplify later stages in the system, all words are stripped of any punctuation and converted to lower case to form a dictionary of accepted words. Once the words have been isolated a dictionary is formed by training the system on these words by simply counting the number of occurences of each word found in the training texts and inserting them into a key-value store. This key-value store ensures constant-time, quick lookups to ensure efficiency. Words that occur less than 40 times are discounted from the dictionary in order to filter out any spelling errors in the training texts. The counts gained from this processing step are then saved to a file for use at a later time. By writing the values to a file this means that it is not necessary for subsequent executions of the correction system.

\subsection{Spelling Error Detection and Correction Algorithms Execution}
To detect errors in the Tweets the system performs a lookup on the provided dictionary for each word in the Tweet. The simple idea behind this method is that if the word is not in the dictionary, then it must be a spelling error.

For the spelling error correction system for Tweets three algorithms were decided upon: the Noisy Channel Method as described by Peter Norvig \cite{}, the character-level n-gram method as proposed by Ahmed et. al. \cite{}, and the context-sensitive n-gram method. These methods were chosen as they represent a wide selection of the possible spelling error correction methods and have all been described as being effective on general spelling error correction.

\subsubsection{Implementing Noisy Channel Method}
The Noisy Channel method by Peter Norvig works by first generating all the possible ways that an error can be reversed into a correct word by a single edit operation. An edit operation can either be a deletion of one character, the insertion of one character, the replacement of one character by another, or the transposition of two adjacent characters. All of these possible edits are generated by simple string manipulation that does not require expensive computation.

Selecting the actual correction is then done by performing a lookup of each candidate correction in the key-value store and choosing the word with the highest probability based on the number of occurrences found in the training text.

\subsubsection{Implementing the Character-Level N-Gram Method}
To implement the character-level n-gram method, n-grams of size two (also known as bi-grams) were selected due to the fact that most spelling errors are caused by the mistyping of a single character. The bi-grams for the spelling error are calculated using string operations that select sub-strings of size two from the error. The set of bi-grams is then compared to each of the words in the key-value store by way of a n-gram similarity function which calculates the similarity between the error and a possible correction using equation 2. The correction method simply selects the word from the key-value store that is most similar to the error.

\subsubsection{Implementing the Context-Sensitive N-Gram Method}
Calculating the most likely correction using context-sensitive n-grams requires an extra step to the above algorithms. For this algorithm to work, a set of n-grams collocations and their related number of occurrences in the training texts are calculated and stored in another key-value store. Collocations are defined as collections of words that occur together too often to be a coincidence. The context-sensitive n-gram method works in a similar way to the above methods by first generating a set of possible corrections. Once the set of corrections have been calculated, using the same function as the noisy channel method, they are ranked based on the probability of a correction occurring based on the words surrounding the error.

\section{Technologies}
One of the most important design decisions to make when designing a computer system is the language of implementation. Originally when building the first iteration of the system I decided upon choosing Java to implement the system. I made this initial decision as I have strong Java skills and the object-oriented nature of the language, particularly interfaces, fit well with the idea of being able to extend the system easily to add new features. Additionally, Java met the requirement of speed with its highly optimised virtual machine that has been refined over the years to produce an extremely efficient runtime environment. With respect to the data structures and methods required to implement each of the spelling error correction methods, Java includes an efficient key-value store called the HashMap and all of the string operations required.

Unfortunately, as implementation progressed it became increasingly cumbersome to implement the error correction methods with the verbosity of the Java language. For this reason I decided to rewrite the system into Python; another language that I was familiar with. Python's simplicity offered an increase in programming efficiency that allowed for more ideas to be experimented with. In addition, Python's string manipulation methods, particularly the ability to slice a string into arbitrary pieces, was incredibly useful. While Python has a much slower execution time than Java, the productivity gains were much more beneficial to the project.

With the switch to Python as the implementation language of choice it was also possible to introduce the Natural Language Toolkit (NLTK) \cite{} into the system. NLTK provides many common natural language functions that were useful in the implementation of the system, with the collocation finder and word tokeniser being of most use. This dramatically reduced the time to train the system as the functions are efficient and also more accurate than the naive algorithms that were first implemented in the Java version.
